{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\bar Y = W \\times X+\\vec b$\n",
    "\n",
    "$(k \\times m) = (k \\times  n) \\times (n \\times m) + (k \\times 1)$\n",
    "\n",
    "- $\\vec x$ is $n$ dimension vector which represents $n$ features\n",
    "- $X$ is a $(n \\times m)$ dimension tensor, $m$ represents batch size,\n",
    "- $W$ is a $(k \\times n)$ dimension weights matrix of the full connected layer w.r.t. $k$ hidden unites(neurons) or $k$ outputs if it is the last layer\n",
    "- $\\vec b$ is a $k$ dimension bias vector\n",
    "- $\\bar Y$ is a $(k \\times m)$ dimension outputs, $m$ is the batch number, $k$ is the output number of last layer. The bar over $\\bar Y$ represents this is the prediction of the model, which is distinguished with the ground truth $Y^\\ast$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd Python example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'x': 2, 'W': 1}\n"
     ]
    }
   ],
   "source": [
    "class array(object):\n",
    "    \"\"\"Simple Array object that support autodiff.\"\"\"\n",
    "    def __init__(self, value, name=None):\n",
    "        self.value = value\n",
    "        if name:\n",
    "            self.grad = lambda g : {name : g}\n",
    "\n",
    "    def __add__(self, other):\n",
    "        assert isinstance(other, int)\n",
    "        ret = array(self.value + other)\n",
    "        ret.grad = lambda g : self.grad(g)\n",
    "        return ret\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        assert isinstance(other, array)\n",
    "        ret = array(self.value * other.value)\n",
    "        def grad(g):\n",
    "            x = self.grad(g * other.value)\n",
    "            x.update(other.grad(g * self.value))\n",
    "            return x\n",
    "        ret.grad = grad\n",
    "        return ret\n",
    "    \n",
    "x = array(1, 'x')\n",
    "W = array(2, 'W')\n",
    "b = 3\n",
    "\n",
    "y = W * x + b\n",
    "print(y.value)\n",
    "print(y.grad(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd Tensorflow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 23.]]\n",
      "[array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32), array([ 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = tf.placeholder(tf.float32, shape=(10,1))\n",
    "W = tf.Variable(tf.ones([1, 10])*2, tf.float32)\n",
    "b = tf.Variable(tf.ones([1])*3, tf.float32)\n",
    "\n",
    "y = tf.matmul(W, x) + b\n",
    "op_grad = tf.gradients(y, [W, b])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "result, grad = sess.run([y,op_grad], {x:np.ones((10,1),np.float32)})\n",
    "print(result)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd PyTorch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 23\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "\n",
      "    1     1     1     1     1     1     1     1     1     1\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(10,1), requires_grad=False)\n",
    "W = Variable(torch.ones(1,10)*2, requires_grad=True)\n",
    "b = Variable(torch.ones(1)*3, requires_grad=True)\n",
    "\n",
    "y = W.mm(x) + b\n",
    "y.backward()\n",
    "\n",
    "print(y)\n",
    "print(W.grad.data)\n",
    "print(b.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd MxNet example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.]\n",
      "[<NDArray 10 @cpu(0)>, None, <NDArray 1 @cpu(0)>]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "x = mx.symbol.Variable('x')\n",
    "W = mx.symbol.Variable('W')\n",
    "b = mx.symbol.Variable('b')\n",
    "y = mx.symbol.dot(W, x) + b\n",
    "\n",
    "args={'x': mx.nd.ones([10,1]),\n",
    "      'W':mx.nd.ones(10)*2, \n",
    "      'b':mx.nd.ones(1)*3}\n",
    "args_grad={'W':mx.nd.ones(10), #change to `zeros` no impact \n",
    "           'b':mx.nd.ones(1)}  #change to `zeros` no impact \n",
    "\n",
    "y=mx.symbol.MakeLoss(y)\n",
    "exe = y.bind(mx.cpu(), args=args, args_grad=args_grad)# args_grad is necessary\n",
    "result = exe.forward(is_train=True)[0].asnumpy()\n",
    "exe.backward()\n",
    "grad = exe.grad_arrays\n",
    "print(result)\n",
    "print(grad)\n",
    "print(grad[0].asnumpy())\n",
    "print(grad[2].asnumpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
